{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "51c9a7de75c84275c65ca97e23d4a911fc38e246"
   },
   "source": [
    "The dataset is comprised of tab-separated files with phrases from the Rotten Tomatoes dataset. The train/test split has been preserved for the purposes of benchmarking, but the sentences have been shuffled from their original order. Each Sentence has been parsed into many phrases by the Stanford parser. Each phrase has a PhraseId. Each sentence has a SentenceId. Phrases that are repeated (such as short/common words) are only included once in the data.\n",
    "\n",
    "train.tsv contains the phrases and their associated sentiment labels. \n",
    "test.tsv contains just phrases. \n",
    "\n",
    "\n",
    "The sentiment labels are:\n",
    "0 - negative\n",
    "1 - somewhat negative\n",
    "2 - neutral\n",
    "3 - somewhat positive\n",
    "4 - positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "243e4f9b40ab8a69e682a13d0adee4bfaa4acb6c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "print(\"Loading data...\")\n",
    "train = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/train.tsv\", sep=\"\\t\")\n",
    "print(\"Train shape:\", train.shape)\n",
    "test = pd.read_csv(\"../input/movie-review-sentiment-analysis-kernels-only/test.tsv\", sep=\"\\t\")\n",
    "print(\"Test shape:\", test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "129bb2635ee6f2c18ff47dde3df2c268324b541a"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e2eeea8d70a649b106c651734af609dccac869ba"
   },
   "outputs": [],
   "source": [
    "enc = OneHotEncoder(sparse=False)\n",
    "enc.fit(train[\"Sentiment\"].values.reshape(-1, 1))\n",
    "print(\"Number of classes:\", enc.n_values_[0])\n",
    "print(\"Class distribution:\\n{}\".format(train[\"Sentiment\"].value_counts()/train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06791fd967e6bfa9130f86a6317dd3b22f640a61"
   },
   "outputs": [],
   "source": [
    "train[\"Sentiment\"].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "04bea1b921fb53f77ee8b069d864d2c103fec0ff"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train_cv = CountVectorizer()\n",
    "train_cv.fit(train[\"Phrase\"])\n",
    "\n",
    "test_cv = CountVectorizer()\n",
    "test_cv.fit(test[\"Phrase\"])\n",
    "\n",
    "print(\"Train Set Vocabulary Size:\", len(train_cv.vocabulary_))\n",
    "print(\"Test Set Vocabulary Size:\", len(test_cv.vocabulary_))\n",
    "print(\"Number of Words that occur in both:\", len(set(train_cv.vocabulary_.keys()).intersection(set(test_cv.vocabulary_.keys()))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2d0cefa184547a5e02f5f389216e1a496f90e982"
   },
   "source": [
    "** Add Numerical Feature**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b4043b8831b8a42bd946f762375a3a97516dfc2"
   },
   "outputs": [],
   "source": [
    "def add_num_feature_to_df(df):\n",
    "    df[\"phrase_count\"] = df.groupby(\"SentenceId\")[\"Phrase\"].transform(\"count\")\n",
    "    df[\"word_count\"] = df[\"Phrase\"].apply(lambda x: len(x.split()))\n",
    "    df[\"has_upper\"] = df[\"Phrase\"].apply(lambda x: x.lower() != x)\n",
    "    df[\"sentence_end\"] = df[\"Phrase\"].apply(lambda x: x.endswith(\".\"))\n",
    "    df[\"after_comma\"] = df[\"Phrase\"].apply(lambda x: x.startswith(\",\"))\n",
    "    df[\"sentence_start\"] = df[\"Phrase\"].apply(lambda x: \"A\" <= x[0] <= \"Z\")\n",
    "    df[\"Phrase\"] = df[\"Phrase\"].apply(lambda x: x.lower())\n",
    "    return df\n",
    "\n",
    "train = add_num_feature_to_df(train)\n",
    "test = add_num_feature_to_df(test)\n",
    "\n",
    "dense_features = [\"phrase_count\", \"word_count\", \"has_upper\", \"after_comma\", \"sentence_start\", \"sentence_end\"]\n",
    "\n",
    "train.groupby(\"Sentiment\")[dense_features].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "ea42c5a316e953cf6740f2798bd3e9b78d4447b9"
   },
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "572a4bda2d8b1cd035f9c6fffa403e7f1b0fbbe4"
   },
   "source": [
    "**Transfer Learning Using GLOVE Embeddings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3e18956c6b5c5c88e7097c41138a2afb3f4bc1e1"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = \"../input/glove-global-vectors-for-word-representation/glove.6B.100d.txt\"\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "all_words = set(train_cv.vocabulary_.keys()).union(set(test_cv.vocabulary_.keys()))\n",
    "\n",
    "def get_embedding():\n",
    "    embeddings_index = {}\n",
    "    emp_f = open(EMBEDDING_FILE)\n",
    "    for line in emp_f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        if len(values) == EMBEDDING_DIM + 1 and word in all_words:\n",
    "            coefs = np.asarray(values[1:], dtype=\"float32\")\n",
    "            embeddings_index[word] = coefs\n",
    "    emp_f.close()\n",
    "    return embeddings_index\n",
    "\n",
    "embeddings_index = get_embedding()\n",
    "print(\"Number of words that don't exist in GLOVE:\", len(all_words - set(embeddings_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f93b76d710ea94ed48627babc49bc5261647789"
   },
   "source": [
    "**Prepare the sequences for LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "5d2f6c292c46b367b0ae7f98f8a1fbafc163a0e0"
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 70\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(np.append(train[\"Phrase\"].values, test[\"Phrase\"].values))\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "nb_words = len(word_index) + 1\n",
    "embedding_matrix = np.random.rand(nb_words, EMBEDDING_DIM + 2)\n",
    "\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    sent = textblob.TextBlob(word).sentiment\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = np.append(embedding_vector, [sent.polarity, sent.subjectivity])\n",
    "    else:\n",
    "        embedding_matrix[i, -2:] = [sent.polarity, sent.subjectivity]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7f3d8de4369b676ffab460533ace55ed08a10ab"
   },
   "source": [
    "**Define the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "878174c7d497cf794531afbcc8d81b5f59c803dd"
   },
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "def build_model():\n",
    "    embedding_layer = Embedding(nb_words,\n",
    "                                EMBEDDING_DIM + 2,\n",
    "                                weights=[embedding_matrix],\n",
    "                                input_length=MAX_SEQUENCE_LENGTH,\n",
    "                                trainable=True)\n",
    "    dropout = SpatialDropout1D(0.25)\n",
    "    mask_layer = Masking()\n",
    "    lstm_layer = LSTM(200)\n",
    "    \n",
    "    seq_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=\"int32\")\n",
    "    dense_input = Input(shape=(len(dense_features),))\n",
    "    \n",
    "    dense_vector = BatchNormalization()(dense_input)\n",
    "    \n",
    "    phrase_vector = lstm_layer(mask_layer(dropout(embedding_layer(seq_input))))\n",
    "    \n",
    "    \n",
    "    feature_vector = concatenate([phrase_vector, dense_vector])\n",
    "    feature_vector = Dense(150, activation=\"relu\")(feature_vector)\n",
    "    feature_vector = Dense(50, activation=\"relu\")(feature_vector)\n",
    "    \n",
    "    output = Dense(5, activation=\"softmax\")(feature_vector)\n",
    "    \n",
    "    model = Model(inputs=[seq_input, dense_input], outputs=output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6879dab4a2dc2daa0a63ab689a7a579a4c4baa5e"
   },
   "source": [
    "**Train the Model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6fbcaac73703c1d1c67f3ca439e7023e6f0bf2b1"
   },
   "outputs": [],
   "source": [
    "train_seq = pad_sequences(tokenizer.texts_to_sequences(train[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_seq = pad_sequences(tokenizer.texts_to_sequences(test[\"Phrase\"]), maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "0d4cdf9226840afc92b25b0ca75b4ff520dabf7b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_dense = train[dense_features]\n",
    "y_train = enc.transform(train[\"Sentiment\"].values.reshape(-1, 1))\n",
    "\n",
    "print(\"Building the model...\")\n",
    "model = build_model()\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"nadam\", metrics=[\"acc\"])\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_acc\", patience=2, verbose=1)\n",
    "model_save_path = \"./model.hdf5\"\n",
    "model_checkpoint = ModelCheckpoint(model_save_path, monitor='val_acc', save_best_only=True, mode='max', verbose=1)\n",
    "\n",
    "print(\"Training the model...\")\n",
    "model.fit([train_seq, train_dense], y_train, validation_split=0.15,\n",
    "          epochs=15, batch_size=512, shuffle=True, callbacks=[early_stopping, model_checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "51b72d89f24f76c696d3d11523f023a9c3ed4ae3",
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
